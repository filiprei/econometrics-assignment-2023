[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Article Review: Inferring Causal Impact Using Bayesian Structural Time-Series Models",
    "section": "",
    "text": "Organisations often need or benefit from assessing the contribution of various parts to a system as a whole. For example, a government agency may wish to understand which policies had the desired outcome, and a business may wish to understand what decision caused an increase in sales. This type of reasoning is known as causal inference.\nIn this report, I review Brodersen et al. (2015) which presents a practical approach to modelling causal impact in time series data using state space models. The paper was influential as measured by citations and people using the accompanying R package. However, the paper’s reproducibility is lacking as the dataset used as empirical justification is not available and the simulation section included most, but not all the required inputs. To alleviate these issues I have made this report fully reproducible so that the main insights can be verified. This report was prepared using R (R Core Team 2023), quarto for document preparation (Allaire et al. 2022), and R packages by Wickham et al. (2019), Pedersen (2023), Meschiari (2022), and Hester and Bryan (2022).\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(latex2exp)\nlibrary(glue)"
  },
  {
    "objectID": "index.html#the-data-generating-process",
    "href": "index.html#the-data-generating-process",
    "title": "Article Review: Inferring Causal Impact Using Bayesian Structural Time-Series Models",
    "section": "The data generating process",
    "text": "The data generating process\n\n\nCode\n# simulation 1\nmax_t &lt;- (as.Date('2014-06-30') - as.Date('2013-01-01') + 1) |&gt; \n  as.numeric()\nintervention_t &lt;- (as.Date('2014-01-1') - as.Date('2013-01-01') + 1)\nset.seed(42)\nmu0 &lt;- 20\ndat &lt;- data.frame(\n  t = 1:546\n) |&gt;\n  mutate(\n    z1 = sin(t*2*pi/90),\n    z2 = sin(t*2*pi/360)\n  ) |&gt;\n  cross_join(data.frame(\n    simulation_id=1:2^8\n  )) |&gt;\n  cross_join(data.frame(\n    e = c(0, 0.001, 0.01, 0.05, 0.1, 0.25, 1, rep(0.1, 5)),\n    d = c(rep(180, 7), 30 * 1:5)\n  )) |&gt; \n  cross_join(data.frame(\n    alternative_setting=c(F,T)\n  )) |&gt;\n  filter(\n    !alternative_setting |\n      (d==180&e==0.1) |\n      (d==180&e==0.05)\n  ) |&gt;\n  filter(t&lt;=366+d) |&gt;\n  arrange(t) |&gt;\n  group_by(simulation_id, e, d, alternative_setting) |&gt;\n  mutate(\n    beta1 = 1 + cumsum(ifelse(alternative_setting, c(rnorm(n()-90,0,0.01),rnorm(90,0,0.03)), rnorm(n(),0,0.01))),\n    beta2 = 1 + cumsum(ifelse(alternative_setting, c(rnorm(n()-90,0,0.01),rnorm(90,0,0.03)), rnorm(n(),0,0.01))),\n    mu = mu0 + cumsum(rnorm(n(),0,0.1)),\n    yt_ = beta1 * z1 + beta2 * z2 + mu + rnorm(n(),0,0.1),\n    yt = yt_ * (t &lt; 366) + yt_ * (1+e) * (1-(t&lt;366))\n  )\nnsims &lt;- 256\nset.seed(42)\ntotal_iter &lt;- dat |&gt;\n  filter(simulation_id&lt;=nsims,\n         e==0.1 |\n         d==180\n  ) |&gt;\n  group_by(simulation_id, e, d, alternative_setting) |&gt;\n  arrange(t) |&gt;\n  select(simulation_id, e, d, alternative_setting, yt, z1, z2) |&gt;\n  nest() |&gt;\n  ungroup() |&gt;\n  nrow()\ncurrent_iter &lt;- total_iter\nstart_time &lt;- as.numeric(lubridate::now())\n\nif(file.exists('sims1-small.Rds')) {\n  sim_res &lt;- readRDS('sims1-small.Rds')\n} else if(!file.exists('sims.Rds')) {\n  sim_res &lt;- dat |&gt;\n    filter(simulation_id&lt;=nsims,\n           e==0.1 |\n             d==180\n    ) |&gt;\n    group_by(simulation_id, e, d, alternative_setting) |&gt;\n    arrange(t) |&gt;\n    select(simulation_id, e, d, alternative_setting, yt, z1, z2) |&gt;\n    nest() |&gt;\n    ungroup() |&gt;\n    rowwise() |&gt;\n    mutate(\n      res = list(\n        (\\(data) {\n          ci &lt;- CausalImpact::CausalImpact(data,\n                                     pre.period=c(1,365),\n                                     post.period=c(366,366+d),\n                                     model.args = list(dynamic.regression=T,\n                                                       prior.level.sd=0.1,\n                                                       niter=1000))\n          \n          cat('Iterations remaining: ', current_iter, '\\n')\n          current_iter &lt;&lt;- current_iter - 1\n          iter_rate &lt;- (as.numeric(lubridate::now()) - start_time)/(total_iter - current_iter)\n          cat('Sec per iter: ', round(iter_rate, 2),'\\n')\n          cat('Est remaining min: ', iter_rate*current_iter/60, '\\n')\n          cbind(Average = ci$summary['Average', ],\n                Cumulative = ci$summary['Cumulative', ]) |&gt;\n            as_tibble() |&gt;\n            mutate(Series = list(as.data.frame(ci$series)))\n        })(data)\n      )\n    )\n  saveRDS(sim_res, 'sims.Rds')\n} else {\n  sim_res &lt;- readRDS('sims.Rds')\n}\nif(!file.exists('sims1-small.Rds')) {\n  sim_res &lt;- sim_res |&gt;\n    mutate(res = ifelse(\n      !(d == 180 & e == .1 &\n          simulation_id == 1 & !alternative_setting),\n      list(res |&gt; select(-Series)),\n      list(res)\n    )) |&gt;\n    select(-data)\n  saveRDS(sim_res, 'sims1-small.Rds')\n}\n\n\nIn the simulation section of Brodersen the data is simulated from 1st of January 2013 to 30th of June 2014, with a causal effect at 1st of January 2014. Equivalently, data can be generated from a time series with \\(t\\in T=\\{1,2,\\ldots,546\\}\\) and an intervention at t=366. In Brodersen et al. (2015) the simulations are intended to represent ad campaigns, although there appears to be no aspect of the simulation study that is specific to that application.\nBrodersen uses the following data generating process,\n\\[\n\\begin{split}\ny_t &= \\beta_{t,1}x_{t,1}+\\beta_{t,2}x_{t,2}+\\mu_t+\\epsilon_t\\\\\n\\beta_{t,i} &\\sim \\mathcal{N}(\\beta_{t-1,i},0.01^2);\\quad \\beta_{0,i}=0; \\quad i \\in \\{1,2\\}\\\\\n\\mu_t &\\sim \\mathcal{N}(\\mu_{t-1},0.1^2);\\quad\\mu_0=20\\\\\n\\epsilon_t &\\sim \\mathcal{N}(0,0.1^2).\n\\end{split}\n\\]\nBased on visual inspection of Figure 3 (a) in Brodersen et al. (2015) it appears that \\(\\mu_0\\approx20\\), not \\(\\mu_0=0\\) as stated in the text. A positive series is necessary for a multiplicative effect to have a meaningful interpretation for ad campaign outcomes such as clicks or sales, therefore I use \\(\\mu_0=20\\), which appears to be the intended value. Another point of ambiguity is that in Brodersen et al. (2015) the particular kind of sinusoid covariate that is used for \\(x_{1}\\) and \\(x_{2}\\) is not specified, only their period. This means that the data generating process is technically not specified in Brodersen et al. (2015) and so any reproduction can’t expect the same results.\nBrodersen et al. (2015) applies a multiplicative factor to imitate a causal effect so that the final observations are given by \\(y^*_t=y_t \\mathbb{I}\\{t&lt;366\\}+y_t(1+e)(1-\\mathbb{I}\\{t&lt;366\\})\\), where \\(\\mathbb{I}\\{f(t)\\}\\) is the indicator function that evaluates to 1 when \\(t \\in \\{w : f(w)\\}\\) and 0 otherwise. It is easy to see that the simulation would be affected by the magnitude of \\(y_t\\) since this imposed shock is relative. In reality many changes are gradual so the approach used by Brodersen et al. (2015) in simulations may be unrealistic. This intuition is visually supported by Figure 2, which shows how a simulated series would be shifted based on different effect sizes.\n\n\nCode\ndat |&gt;\n  ungroup() |&gt;\n  filter(e==0.1,d==180,!alternative_setting) |&gt;\n  filter(simulation_id==1) |&gt;\n  select(-e) |&gt;\n  cross_join(data.frame(\n    e=c(0, 0.1,0.25,1)\n  )) |&gt;\n  mutate(\n    yt = yt_ * (1 + ifelse(t &lt; 366, 0, e))\n  ) |&gt;\n  (\\(x) {\n    ggplot(x,aes(\n    t,yt, group=interaction(e,t &lt; 366)\n  )) +\n    geom_line() +\n    geom_text(aes(label=glue('e = {scales::label_percent(1)(e)}')), \n              data=x |&gt; filter(t==max(t)),\n              hjust=0, nudge_x = 10) +\n    expand_limits(y=0,x=630) +\n    geom_vline(xintercept = 365.5, alpha=.25) +\n    labs(y=TeX('$y_t$'))\n  })(x=_)\n\n\n\n\n\n\n\n\nFigure 2: Examples of how Brodersen et al. (2015) applies a multiplicative factor in simulations to imitate the effect of a sustained intervention effect.\n\n\n\n\n\nIn Brodersen et al. (2015), the time series was simulated in 256 times for each effect size \\(e\\in\\{0,0.001,0.01,0.1,1\\}\\). Although, Figure 3 (b) actually shows effect sizes 25% and 50% which were not mentioned in the text. I opted to simulate \\(e\\in\\{0,0.001,0.01,0.05, 0.1, 0.25 ,1\\}\\). Additionally for \\(e=0.1\\) the time series was simulated 256 times for different campaign durations \\(\\max T - 366 \\in \\{30,60,90,120,150,180\\}\\) to study the coverage properties of changing the campaign duration."
  },
  {
    "objectID": "index.html#model-fitting",
    "href": "index.html#model-fitting",
    "title": "Article Review: Inferring Causal Impact Using Bayesian Structural Time-Series Models",
    "section": "Model fitting",
    "text": "Model fitting\nFor illustrative purposes, a simulation realisation is visualised in Figure 3 where the true effect size was 0.1 and the ad campaign was 180 days. The same realisation was used to fit the CausalImpact model proposed by Brodersen which is visualised in Figure 4. Up until the campaign the model fits very well so it is not unreasonable to expect that the series \\(X_1\\) and \\(X_2\\) could predict \\(Y_t(0)\\) once the intervention period has started. As is the case for regular forecasting, the counterfactual forecasts also get increasingly uncertain further past the intervention date.\n\n\nCode\nold_names &lt;- c('beta1', 'beta2', 'mu',\n               'z1', 'z2', 'yt')\nexample_filter &lt;- \\(x) filter(x, d == 180, e == .1,\n                              simulation_id == 1,!alternative_setting)\nexample_dat_long &lt;- dat |&gt;\n  example_filter() |&gt;\n  pivot_longer(-c(t, simulation_id)) |&gt;\n  filter(name %in% old_names) |&gt;\n  mutate(name = factor(name, levels = old_names))\nlevels(example_dat_long$name) &lt;- c(\n  'beta1' = TeX('$\\\\beta_{t,1}$'),\n  'beta2' = TeX('$\\\\beta_{t,2}$'),\n  'mu' = TeX('$\\\\mu_t$'),\n  'z1' = TeX('$x_{t,1}$'),\n  'z2' = TeX('$x_{t,2}$'),\n  'yt' = TeX('$y_t$')\n)\nexample_dat_long |&gt;\n  ggplot(aes(t,value,group=simulation_id)) +\n    facet_wrap(~name, scales='free_y', ncol=2, \n               labeller = label_parsed) +\n    geom_line(lwd=.25) + \n    theme(legend.position = 'none',\n          axis.title.y.left = element_blank())\n\n\n\n\n\n\n\n\nFigure 3: An example simulation realisation with effect size 0.1 and intervention, e.g. ad campaign, lasting 180 days.\n\n\n\n\n\n\n\nCode\np1 &lt;- sim_res |&gt;\n  example_filter() |&gt;\n  unnest(res) |&gt;\n  select(Series) |&gt;\n  unnest(Series) |&gt;\n  rowid_to_column('t') |&gt;\n  (\\(.)\n  ggplot(.,aes(t, )) +\n    geom_ribbon(aes(ymin=point.pred.lower, ymax=point.pred.upper), alpha=.75, fill='lightblue') +\n    geom_line(aes(y=point.pred), color='darkblue') +\n    geom_line(aes(y=response)) +\n    ggrepel::geom_text_repel(aes(y=point.pred), box.padding = .2, direction = 'y', nudge_x = 30, label=TeX('$\\\\hat{Y_t(0)}$'), color='darkblue', data=.|&gt;slice_max(t)) +\n    ggrepel::geom_text_repel(aes(y=response), box.padding = .2, direction = 'y', nudge_x = 30, label=TeX('$Y_t(1)$'), data=.|&gt;slice_max(t)) +\n    labs(x='t', y='Response unit (e.g. clicks)') +\n    geom_vline(xintercept = 365.5, alpha=.25) +\n    theme_bw()\n  )(.=_)\np2 &lt;- sim_res |&gt;\n  example_filter() |&gt;\n  unnest(res) |&gt;\n  select(Series) |&gt;\n  unnest(Series) |&gt;\n  rowid_to_column('t') |&gt;\n  mutate(true_effect = ifelse(t&gt;=366,.1,0)*response) |&gt;\n  (\\(.)\n  ggplot(.,aes(t, point.effect)) +\n    geom_ribbon(aes(ymin=point.effect.lower, ymax=point.effect.upper), alpha=.75, fill='lightblue') +\n    geom_line() +\n    geom_line(aes(y=true_effect), lty=2) +\n    labs(x='t', y='Point causal effect') +\n    geom_hline(yintercept = 0, alpha=.25) +\n    geom_vline(xintercept = 365.5, alpha=.25) +\n    theme_bw()\n  )(.=_)\np3 &lt;- sim_res |&gt;\n  example_filter() |&gt;\n  unnest(res) |&gt;\n  select(Series) |&gt;\n  unnest(Series) |&gt;\n  rowid_to_column('t') |&gt;\n  mutate(true_effect = cumsum(ifelse(t&gt;=366,response*.1,0))) |&gt;\n  (\\(.)\n  ggplot(.,aes(t, cum.effect)) +\n    geom_ribbon(aes(ymin=cum.effect.lower, ymax=cum.effect.upper), alpha=.75, fill='lightblue') +\n    geom_line() +\n    geom_line(aes(y=true_effect), lty=2) +\n    geom_hline(yintercept = 0, alpha=.25) +\n    geom_vline(xintercept = 365.5, alpha=.25) +\n    labs(x='t', y='Cumulative causal effect') +\n    theme_bw()\n  )(.=_)\n\n\n\n\nCode\n(p1 + labs(title='(A)')) / (p2 + labs(title='(B)')) / (p3 + labs(title='(C)'))\n\n\n\n\n\n\n\n\nFigure 4: (A) Using the same simulation realisation as in Figure 3, the response is compared to a retrospectively forecasted counterfactual, \\(\\hat{Y}_t(0)\\), along with a 95% credibility interval which gets wider as the time since the start of the campaign increases. (B) The point estimate causal effect based on the observed response less the forecasted counterfactual. Dashed line shows ground truth (by construction). (C) The estimated cumulative causal effect defined as the sum of point causal effects."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Article Review: Inferring Causal Impact Using Bayesian Structural Time-Series Models",
    "section": "Results",
    "text": "Results\nOne of the seemingly key plots in the Brodersen paper, Figure 3 (b), is reproduced in Figure 5 (A), using inputs ascertained from the simulation section of Brodersen et al. (2015). The plot shows the proportion of simulations in each of the effect size settings that rejected the null hypothesis of no causal impact in the positive direction. The null was rejected when the lower bound of the 95% credible interval for the average effect was above zero. As noted in Brodersen et al. (2015), this provides an estimate of sensitivity . The results are different to those of Brodersen, although this is not surprising given the lack of specific information around covariates used by Brodersen et al. (2015). Higher effect sizes are more likely to picked up, but the power will depend on the particular data generating process, in particular the amount of noise.\nFigure 5 (B) confirms that for the most part the coverage probability is reasonable for different campaign durations. It also shows how the Bayesian approach leads to a shrinkage estimator, which on the whole provides a similar result qualitatively.\nFigure 6 shows the impact on estimation accuracy of a change to standard deviation of dynamic coefficients 90 days into the treatment period. The situation considered is a 180 day treatment period and an effect size e=10%. The simulation was run 64 times for each setting. Stated mathematically, for some new standard deviation \\(c\\), the coefficients evolve according to,\n\\[\n\\beta_{t,i} \\sim\n\\begin{cases}\n\\mathcal{N}(\\beta_{t-1,i},\\ 0.01^2) &\\text{if } t &lt; 366+90\\\\\n\\mathcal{N}(\\beta_{t-1,i},\\ c^2) &\\text{otherwise}.\n\\end{cases}\n\\]\nThe figure shows that for a small change to \\(c\\) there is not a meaningful deterioration of estimation accuracy. This differs from Figure 4 (a) in Brodersen et al. (2015). The discrepancy can be attributed to this reproduction using unit sinusoids for the covariates while Brodersen et al. (2015) appear to be using something else, otherwise the estimation error would periodically converge with the no structural change case when covariates are zero. However, instead of guessing what Brodersen et al. (2015) may have meant, I use the most reasonable interpretation of the text. The structural change with a new standard deviation of 0.3 does however show a similar increase in relative absolute error to that in Brodersen et al. (2015), but clearly with much more influence from the sinusoidal nature of the covariate.\n\n\nCode\nfig3_p1 &lt;- sim_res |&gt;\n  filter(!alternative_setting, d==180) |&gt;\n  unnest(res) |&gt;\n  select(where(negate(is.list))) |&gt;\n  group_by(e, d) |&gt;\n  summarise(\n    reject = sum(0 &lt; Average.RelEffect.lower),\n    not_reject = n() - reject,\n    .groups='drop'\n  ) |&gt;\n  mutate(\n    alpha = 1 + reject,\n    beta = 1 + not_reject,\n    ll = qbeta(.025, alpha, beta),\n    ul = qbeta(.975, alpha, beta),\n    rejection_rate = alpha/(alpha + beta)\n  ) |&gt;\n  mutate(Group = 'Bayesian') |&gt;\n  bind_rows(\n    sim_res |&gt;\n  filter(!alternative_setting, d==180) |&gt;\n  unnest(res) |&gt;\n  select(where(negate(is.list))) |&gt;\n  group_by(e, d) |&gt;\n  summarise(\n    rejection_rate = mean(0 &lt; Average.RelEffect.lower),\n    sd_err = sqrt(rejection_rate * (1-rejection_rate) / n()),\n    ll = rejection_rate - 1.96 * sd_err,\n    ul = rejection_rate + 1.96 * sd_err,\n    .groups = 'drop'\n  ) |&gt;\n    mutate(Group = 'Frequentist')\n  ) |&gt;\n  (\\(x)\n  ggplot(x,aes(x=factor(e), y=rejection_rate, ymin=ll, ymax=ul,\n             color=Group)) +\n    geom_pointrange(pch=1, position = position_dodge2(.3)) +\n    geom_text(aes(label=Group), data=x|&gt;filter(e==.1),\n              angle=90, position = position_dodge2(1.25),\n              vjust=.5) +\n    scale_x_discrete(labels=\\(x)scales::label_percent(.1)(as.numeric(x))) +\n    scale_y_continuous(labels=scales::label_percent(1)) +\n    scale_colour_manual(values = c(Bayesian='blue', Frequentist='darkgreen')) +\n    labs(x='True relative effect (e)', y='Rejection rate',\n         title='(A)') +\n    theme(legend.position = 'none'))(x=_)\nfig3_p2 &lt;- sim_res |&gt;\n  filter(!alternative_setting,e==.1) |&gt;\n  unnest(res) |&gt;\n  select(where(negate(is.list))) |&gt;\n  group_by(d) |&gt;\n  summarise(\n    covered = sum(between(e, Average.RelEffect.lower, Average.RelEffect.upper)),\n    not_covered = n() - sum(between(e, Average.RelEffect.lower, Average.RelEffect.upper)),\n    .groups = 'drop'\n  ) |&gt;\n  mutate(\n    alpha = 1 + covered,\n    beta = 1 + not_covered,\n    ll = qbeta(.025, alpha, beta),\n    ul = qbeta(.975, alpha, beta),\n    p = alpha/(alpha + beta)\n  ) |&gt;\n  mutate(Group = 'Bayesian') |&gt;\n  bind_rows(\n    sim_res |&gt;\n      filter(!alternative_setting, e == .1) |&gt;\n      unnest(res) |&gt;\n      select(where(negate(is.list))) |&gt;\n      group_by(d) |&gt;\n      summarise(\n        p = mean(between(\n          e, Average.RelEffect.lower, Average.RelEffect.upper\n        )),\n        sd_err = sqrt(p * (1 - p) / n())\n      ) |&gt;\n      mutate(\n        ll = p - 1.96 * sd_err,\n        ul = p + 1.96 * sd_err,\n        .groups = 'drop'\n      ) |&gt;\n      mutate(Group = 'Frequentist')\n  ) |&gt;\n  (\\(x)\n  ggplot(x,aes(x=factor(d), y=p, ymin=ll, ymax=ul,\n             color=Group)) +\n    geom_pointrange(pch=1, position = position_dodge2(.3)) +\n    geom_text(aes(label=Group), data=x|&gt;filter(d==90),\n              angle=90, position = position_dodge2(1.25),\n              vjust=.5) +\n    scale_x_discrete() +\n    scale_y_continuous(labels=scales::label_percent(1)) +\n    scale_colour_manual(values = c(Bayesian='blue', Frequentist='darkgreen')) +\n    geom_hline(yintercept = .95, lty=2) +\n    labs(x='Campaign duration', y='Coverage probability',\n         title='(B)') +\n    theme(legend.position = 'none'))(x=_)\nfig3_p1 / fig3_p2\n\n\n\n\n\n\n\n\nFigure 5: (A) The empirical prevalence of credibility intervals that exclude zero in the positive direction, i.e., a power curve, for the simulation setting with a 180 day intervention period. (B) The empirical coverage of causal effect credibility interval for simulations of different campaign durations. Frequentist 95% confidence interval estimated by \\(\\hat{p} \\pm 1.96 \\sqrt{\\hat{p} (1-\\hat{p}) / n}\\). Bayesian 95% credibility interval determined with a uniform(0,1) prior. The point estimate in the frequentist case is the MLE \\(\\hat p\\) and in the Bayesian case the posterior expectation.\n\n\n\n\n\n\n\nCode\n# simulation 2\nmax_t &lt;- (as.Date('2014-06-30') - as.Date('2013-01-01') + 1) |&gt; \n  as.numeric()\nintervention_t &lt;- (as.Date('2014-01-1') - as.Date('2013-01-01') + 1)\nset.seed(42)\nmu0 &lt;- 20\ndat2 &lt;- data.frame(\n  t = 1:546\n) |&gt;\n  mutate(\n    z1 = sin(t*2*pi/90),\n    z2 = sin(t*2*pi/360)\n  ) |&gt;\n  cross_join(data.frame(\n    simulation_id=1:2^8\n  )) |&gt;\n  cross_join(data.frame(\n    e = c(0.1),\n    d = c(180),\n    new_sd = c(0.01,0.1, 0.3)\n  )) |&gt; \n  filter(t&lt;=366+d) |&gt;\n  arrange(t) |&gt;\n  group_by(simulation_id, e, d, new_sd) |&gt;\n  mutate(\n    beta1 = 1 + cumsum(c(rnorm(n()-90,0,0.01),rnorm(90,0,new_sd))),\n    beta2 = 1 + cumsum(c(rnorm(n()-90,0,0.01),rnorm(90,0,new_sd))),\n    mu = mu0 + cumsum(rnorm(n(),0,0.1)),\n    yt_ = beta1 * z1 + beta2 * z2 + mu + rnorm(n(),0,0.1),\n    yt = yt_ * (t &lt; 366) + yt_ * (1+e) * (1-(t&lt;366))\n  )\nnsims &lt;- 64\nset.seed(42)\ntotal_iter &lt;- dat2 |&gt;\n  filter(simulation_id&lt;=nsims) |&gt;\n  group_by(simulation_id, e, d, new_sd) |&gt;\n  arrange(t) |&gt;\n  select(simulation_id, e, d, new_sd, yt, z1, z2) |&gt;\n  nest() |&gt;\n  ungroup() |&gt;\n  nrow()\ncurrent_iter &lt;- total_iter\nstart_time &lt;- as.numeric(lubridate::now())\nif(file.exists('sims2-small.Rds')) {\n  sim_res2 &lt;- readRDS('sims2-small.Rds')\n} else if(!file.exists('sims2.Rds')) {\n  sim_res2 &lt;- dat2 |&gt;\n    filter(simulation_id&lt;=nsims) |&gt;\n    group_by(simulation_id, e, d, new_sd) |&gt;\n    arrange(t) |&gt;\n    select(simulation_id, e, d, new_sd, yt, z1, z2) |&gt;\n    nest() |&gt;\n    ungroup() |&gt;\n    rowwise() |&gt;\n    mutate(\n      res = list(\n        (\\(data) {\n          ci &lt;- CausalImpact::CausalImpact(data,\n                                     pre.period=c(1,365),\n                                     post.period=c(366,366+d),\n                                     model.args = list(dynamic.regression=T,\n                                                       prior.level.sd=0.1,\n                                                       niter=1000))\n          \n          cat('Iterations remaining: ', current_iter, '\\n')\n          current_iter &lt;&lt;- current_iter - 1\n          iter_rate &lt;- (as.numeric(lubridate::now()) - start_time)/(total_iter - current_iter)\n          cat('Sec per iter: ', round(iter_rate, 2),'\\n')\n          cat('Est remaining min: ', iter_rate*current_iter/60, '\\n')\n          cbind(Average = ci$summary['Average', ],\n                Cumulative = ci$summary['Cumulative', ]) |&gt;\n            as_tibble() |&gt;\n            mutate(Series = list(as.data.frame(ci$series)))\n        })(data)\n      )\n    )\n  saveRDS(sim_res2, 'sims2.Rds')\n} else {\n  sim_res2 &lt;- readRDS('sims2.Rds')\n}\nif(!file.exists('sims2-small.Rds')) {\n  sim_res2 |&gt;\n    select(-data) |&gt;\n    mutate(res = list(res |&gt; select(Series))) |&gt;\n    unnest(res) |&gt;\n    rowwise() |&gt;\n    mutate(Series = list(Series |&gt; select(point.effect,response))) |&gt;\n    saveRDS('sims2-small.Rds')\n}\n\n\n\n\nCode\nsim_res2 |&gt;\n  mutate(Series = list(rowid_to_column(Series, 't'))) |&gt;\n  unnest(Series) |&gt;\n  mutate(\n    true_effect = response / (1 + e) * e,\n    err = abs(point.effect - true_effect) / true_effect\n  ) |&gt;\n  group_by(t, new_sd) |&gt;\n  summarise(\n    mean_err = mean(err),\n    sd_err = sd(err) / sqrt(n()),\n    ll = mean_err - 1.96 * sd_err,\n    ul = mean_err + 1.96 * sd_err,\n    .groups = 'drop'\n  ) |&gt;\n  filter(t&gt;365) |&gt;\n  ggplot(aes(t, mean_err, color = factor(new_sd), group = factor(new_sd))) +\n  geom_ribbon(aes(ymin = ll, ymax = ul, fill = factor(new_sd), color=NULL),\n              alpha = .25) +\n  geom_line() +\n  geom_vline(xintercept = max_t - 90, lty=2, alpha=.25) +\n  labs(color='New SD',\n       fill='New SD',\n       y='Mean absolute % error') +\n  scale_y_continuous(labels = scales::label_percent(1)) +\n  theme(legend.position = 'top')\n\n\n\n\n\n\n\n\nFigure 6: The absolute percentage error for pointwise treatment effect compared to the constructed true effect for a structural change in the coefficient dynamics. Vertical line indicates structural change. Shaded 95% confidence intervals estimated by \\(\\hat\\mu\\pm 1.96\\hat\\sigma/\\sqrt{64}\\) at each time point, where \\(\\hat\\mu\\) is the sample mean absolute % error, and \\(\\hat\\sigma\\) is the sample standard deviation of the mean absolute % error. Note that a new SD of 0.01 corresponds to no structural change.\n\n\n\n\n\n\n\nCode\nwriteLines(\n  glue(\n    '{paste0(capture.output(sessionInfo()), collapse=\"\\n\")}\\n\\nquarto version:\\n {capture.output(quarto::quarto_version())[[1]]}'\n  ), \"sessionInfo.txt\")"
  }
]